{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Introduction to LangChain\n",
    "\n",
    "## Setup\n",
    "#### Follow [README](https://github.com/tirtho/open-ai/blob/main/README.md) and perform setup before running the notebooks\n",
    "\n",
    "Reference : \n",
    "- [Azure Open AI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview)\n",
    "- [LangChain home page](https://python.langchain.com/docs/get_started/introduction.html)\n",
    "\n",
    "#### Load the API key and relevant Python libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1685732595135
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got Azure OpenAI Credentials from Azure Key Vault with Azure CLI Auth\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import sys\n",
    "\n",
    "from azure_openai_setup import set_openai_config, get_openai_global_config_parameters \n",
    "\n",
    "set_openai_config()\n",
    "\n",
    "theOpenAIParams, modelName, modelDeploymentName = get_openai_global_config_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Simple Chatbot example\n",
    "The below example makes direct LangChain library calls so it is better \\\n",
    "for us to understand this LangChain framework.\\\n",
    "In subsequent examples afterwards we will create and use a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# The openai.<variables> are already filled up by the above \n",
    "# set_openai_config() helper function called from above cell.\n",
    "# Check that function for more details\n",
    "\n",
    "aChat = AzureChatOpenAI(\n",
    "            openai_api_key = theOpenAIParams.api_key,\n",
    "            openai_api_base = theOpenAIParams.api_base,\n",
    "            openai_api_version = theOpenAIParams.api_version,\n",
    "            deployment_name = modelDeploymentName,\n",
    "            temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asking bot to perform as task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "aChat([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate completions for multiple sets of messaages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'adore la programmation.\n",
      "J'adore l'intelligence artificielle.\n"
     ]
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result = aChat.generate(batch_messages)\n",
    "for generation in result.generations:\n",
    "    print(generation[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "The concept of templates make life so much easier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Prompt Engineer creates the ChatPrompt object\n",
    "\n",
    "and hands over the aChatPrompt to your Application Dev Engineer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"You are a helpful assistance that translates to {target_language}.\"\n",
    "aSystemPrompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_template = \"{text}\"\n",
    "aHumanPrompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "aChatPrompt = ChatPromptTemplate.from_messages([aSystemPrompt, aHumanPrompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your App Dev Engineer receives aChatPrompt\n",
    "Populates that prompt with user input and receives completion from the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "theCompletion = aChat(aChatPrompt.format_prompt(target_language=\"English Pirate\", \n",
    "                                text=\"Hello Sir! Would you like to buy a bottle of rum from \\\n",
    "                                the Captain of the Black Pearl?\").to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy matey! Aye, I would be interested in purchasing a bottle of rum from the Captain of the Black Pearl. Pray tell, what be the price for such a fine bottle of grog?\n"
     ]
    }
   ],
   "source": [
    "print(theCompletion.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser\n",
    "Helps you get the completion parsed in your desired format. Different types of output parsers are\n",
    "\n",
    "- PydanticOutputParser\n",
    "- CommaSeparatedListOutputParser\n",
    "- Datetime\n",
    "- Enum\n",
    "- OutputFixingParser\n",
    "- RetryOutputParser\n",
    "- Structured Output Parser\n",
    "\n",
    "Let's checkout the Structured Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take this actual quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_quote_struct = {\n",
    "    'auto': True,\n",
    "    'premium': '$200',\n",
    "    'driver': 'TR BARARI',\n",
    "    'vin': 'JH4DA9460LS000685',\n",
    "    'liability':'$100000',\n",
    "    'collision_deduct':'$250',\n",
    "    'comprehensive_deduct':'$100',\n",
    "    'personal_injury':True,\n",
    "    'uninsured':True,\n",
    "    'underinsured':True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is the example quote in an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_insurance_quote_text = f\"\"\"\n",
    "Dear Mr TR Barari, \\\n",
    "\n",
    "I would like to take this opportunity to congratulate you \\\n",
    "on being approved for your auto insurance from Reliable Insurance Inc. \\\n",
    " \\\n",
    "Here are the details of the quote. \\\n",
    " \\\n",
    "    Monthly premium is $200 \\\n",
    "    Driver Name is TR BARARI \\\n",
    "    Auto VIN Number is JH4DA9460LS000685 \\\n",
    "    Coverage for Liability Insurance is $100000 \\\n",
    "    Collision is covered with a per claim deductible of $250 \\\n",
    "    Comprehensive insurance is included with a per claim deductible of $100 \\\n",
    "    Personal_injury is covered \\\n",
    "    Includes full coverage against uninsured and underinsured motorist \\\n",
    "    Coverage Start Date 06/01/2023 \\\n",
    " \\\n",
    "Please email back with your acceptance of this offer in the next 3 days \\\n",
    "for this to be effective. \\\n",
    " \\\n",
    "Regards \\\n",
    " \\\n",
    "John Doe \\\n",
    "Vice President \\\n",
    "Reliable Insurance Inc.\\\n",
    "1 Main Street, NY 12345 \\\n",
    "Ph: +1 (234) 567-9876\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The template from the Prompt Engineer with instructions to process quote emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_insurance_quote_template = \"\"\"\n",
    "Extract from {text} based on the instructions in {format_instructions}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=auto_insurance_quote_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's use the LangChain output parser to parse insurance quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "auto_schema = ResponseSchema(name=\"auto\",\n",
    "                             description=\"Was this an Auto Insurance Quote? Answer as True or False\")\n",
    "premium_schema = ResponseSchema(name=\"premium\",\n",
    "                             description=\"How much is the monthly premium?\")\n",
    "driver_schema = ResponseSchema(name=\"driver\",\n",
    "                             description=\"What is the name of the driver?\")\n",
    "vin_schema = ResponseSchema(name=\"vin\",\n",
    "                             description=\"What is the VIN Number of the auto?\")\n",
    "liability_schema = ResponseSchema(name=\"liability\",\n",
    "                             description=\"How much is the liability coverage?\")\n",
    "collision_deduct_schema = ResponseSchema(name=\"collision_deduct\",\n",
    "                             description=\"How much is the collision deductible per claim?\")\n",
    "comprehensive_deduct_schema = ResponseSchema(name=\"comprehensive_deduct\",\n",
    "                             description=\"How much is the comprehensive deductible per claim?\")\n",
    "uninsured_schema = ResponseSchema(name=\"uninsured\",\n",
    "                             description=\"Is uninsured motorist coverage included? Answer as True or False\")\n",
    "underinsured_schema = ResponseSchema(name=\"underinsured\",\n",
    "                             description=\"Is underinsured motorist coverage included? Answer as True or False\")\n",
    "\n",
    "insurance_quote_schemas = [auto_schema, premium_schema, driver_schema, \n",
    "                    vin_schema, liability_schema, collision_deduct_schema, \n",
    "                    comprehensive_deduct_schema, uninsured_schema, underinsured_schema]\n",
    "\n",
    "insurance_quote_parser = StructuredOutputParser.from_response_schemas(insurance_quote_schemas)\n",
    "\n",
    "auto_insurance_quote_format_instructions = insurance_quote_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developer creates the message, sends it to the model and gets the response back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt.format_messages(text=auto_insurance_quote_text,\n",
    "                                 format_instructions=auto_insurance_quote_format_instructions)\n",
    "response_from_model = aChat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the LangChain parser to parse the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_quote_struct = insurance_quote_parser.parse(response_from_model.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if the expected quote is same as the quote extracted by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: The expected quote is same as the output quote extracted by the Azure OpenAI model\n"
     ]
    }
   ],
   "source": [
    "# Check if the quoted $ is same in both\n",
    "\n",
    "if output_quote_struct.get('premium') == actual_quote_struct.get('premium'):\n",
    "    print(\"SUCCESS: The expected quote is same as the output quote extracted by the Azure OpenAI model\")\n",
    "else:\n",
    "    print(\"FAILED: The expected and output quote are not same\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
