{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Introduction to Prompting\n",
    "\n",
    "## Setup\n",
    "#### Before you start these notebooks, make sure you have the OS environment variables setup. Example provided in the env.bat file for Windows in the root folder. Default variable settings for getting the OpenAI keys from your Azure Key Vault are \n",
    "- PYTHONPATH=%cd%\\AzureOpenAIHelperFunctions\n",
    "- KEY_VAULT_URL=https://\\<YOUR AZURE KEY VAULT NAME\\>.vault.azure.net/\n",
    "- OPENAI_AUTH_TYPE=KeysFromAKVWithCLIAuth\n",
    "#### The different options for OPENAI_AUTH_TYPE are - \n",
    "- KeysFromEnv => get the credentials from OS env\n",
    "- KeysFromAKVWithCLIAuth => get the credentials from Azure Key Vault with CLI Auth\n",
    "- KeysFromAKVWithMI => get the credentials from Azure Key Vault with Managed Identity\n",
    "- KeysFromManagedId => authenticate with Managed Identity and get access to Azure Open AI\n",
    "#### Load the API key and relevant Python libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1685732595135
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got Azure OpenAI Credentials from Azure Key Vault with Azure CLI Auth\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import sys\n",
    "\n",
    "from azure_openai_setup import set_openai_config, get_completion, COMPLETION_MODEL_DEPLOYMENT_NAME\n",
    "\n",
    "set_openai_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Simple Chatbot example\n",
    "The below example makes direct LangChain library calls so it is better \\\n",
    "for us to understand this LangChain framework.\\\n",
    "In subsequent examples afterwards we will create and use a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# The openai.<variables> are already filled up by the above \n",
    "# set_openai_config() helper function called from above cell.\n",
    "# Check that function for more details\n",
    "\n",
    "aChat = AzureChatOpenAI(\n",
    "            openai_api_key = openai.api_key,\n",
    "            openai_api_base = openai.api_base,\n",
    "            openai_api_version = openai.api_version,\n",
    "            deployment_name = COMPLETION_MODEL_DEPLOYMENT_NAME,\n",
    "            temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asking bot to perform as task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "aChat([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate completions for multiple sets of messaages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'adore la programmation.\n",
      "J'adore l'intelligence artificielle.\n"
     ]
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result = aChat.generate(batch_messages)\n",
    "for generation in result.generations:\n",
    "    print(generation[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "The concept of templates make life so much easier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Prompt Engineer creates the ChatPrompt object\n",
    "\n",
    "and hands over the aChatPrompt to your Application Dev Engineer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"You are a helpful assistance that translates to {target_language}.\"\n",
    "aSystemPrompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_template = \"{text}\"\n",
    "aHumanPrompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "aChatPrompt = ChatPromptTemplate.from_messages([aSystemPrompt, aHumanPrompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your App Dev Engineer receives aChatPrompt\n",
    "Populates that prompt with user input and receives completion from the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "theCompletion = aChat(aChatPrompt.format_prompt(target_language=\"Pirates English\", \n",
    "                                text=\"Hello Sir! Would you be interested in buying a bottle of rum from me, \\\n",
    "                                the Captain of the Black Pearl?\").to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy matey! Aye, I be interested in buying a bottle of rum from ye, the Captain of the Black Pearl. Pray tell, what be the price ye be askin' for it?\n"
     ]
    }
   ],
   "source": [
    "print(theCompletion.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser\n",
    "Helps you get the completion parsed in your desired format. Different types of output parsers are\n",
    "\n",
    "- PydanticOutParser\n",
    "- CommaSeparatedListOutputParser\n",
    "- Datetime\n",
    "- Enum\n",
    "- OutputFixingParser\n",
    "- RetryOutputParser\n",
    "- Structured Output Parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
